{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (3.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nepalitokenizers in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (0.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.3 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from nepalitokenizers) (0.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from tokenizers>=0.13.3->nepalitokenizers) (0.26.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.3->nepalitokenizers) (2024.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece transformers datasets torch scikit-learn\n",
    "%pip install nepalitokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\L E G I O N\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train SentencePiece Tokenizer\n",
    "def train_sentencepiece_tokenizer(data_file, model_prefix, vocab_size=32000):\n",
    "    \"\"\"\n",
    "    Train a SentencePiece tokenizer for the Nepali language.\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=data_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"bpe\",  # Can be \"unigram\", \"bpe\", etc.\n",
    "        character_coverage=0.9995,  # Adjust for Devanagari\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3\n",
    "    )\n",
    "    print(f\"SentencePiece model trained and saved as {model_prefix}.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "input_file = 'C:/Users/L E G I O N/Desktop/Nepali-LLM/data/chunked_nepali_corpus.txt'\n",
    "train_file = 'nepali_train.csv'\n",
    "test_file = 'nepali_test.csv'\n",
    "test_size_ratio = 0.2  # 20% for testing\n",
    "\n",
    "# Process the text file and split into train/test datasets\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(train_file, 'w', encoding='utf-8', newline='') as train_out, \\\n",
    "     open(test_file, 'w', encoding='utf-8', newline='') as test_out:\n",
    "    \n",
    "    train_writer = csv.writer(train_out)\n",
    "    test_writer = csv.writer(test_out)\n",
    "    \n",
    "    # Write headers\n",
    "    train_writer.writerow(['text'])\n",
    "    test_writer.writerow(['text'])\n",
    "    \n",
    "    # Read and process line by line\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line:  # Skip empty lines\n",
    "            if random.random() < test_size_ratio:\n",
    "                test_writer.writerow([line])\n",
    "            else:\n",
    "                train_writer.writerow([line])\n",
    "\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and overwritten nepali_train.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = 'nepali_train.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    rows = list(csv.reader(file))\n",
    "\n",
    "# Filter out rows containing '------'\n",
    "rows = [row for row in rows if '------' not in row]\n",
    "\n",
    "# Overwrite the original file with cleaned data\n",
    "with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Cleaned and overwritten {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and overwritten nepali_test.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = 'nepali_test.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    rows = list(csv.reader(file))\n",
    "\n",
    "# Filter out rows containing '------'\n",
    "rows = [row for row in rows if '------' not in row]\n",
    "\n",
    "# Overwrite the original file with cleaned data\n",
    "with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Cleaned and overwritten {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Dataset\n",
    "def load_nepali_dataset():\n",
    "    \"\"\"\n",
    "    Load or create a Nepali dataset for the classification task.\n",
    "    \"\"\"\n",
    "    # Replace this with actual data loading logic\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": \"nepali_train.csv\", \"test\": \"nepali_test.csv\"})\n",
    "    return DatasetDict({\"train\": dataset[\"train\"], \"test\": dataset[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data and vocab files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nepalitokenizers import WordPiece, SentencePiece\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize tokenizers\n",
    "tokenizer_wp = WordPiece()\n",
    "tokenizer_sp = SentencePiece()\n",
    "file_path='chunkeddataset.txt'\n",
    "# Function to load Nepali text from a .txt file\n",
    "def load_nepali_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Load Nepali text from a .txt file and return a list of sentences.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Read all lines from the file\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Clean up the lines: Remove any unnecessary newlines and empty lines\n",
    "    sentences = [line.strip() for line in lines if line.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Function to tokenize a dataset using a specified tokenizer\n",
    "# Function to tokenize a dataset using a specified tokenizer\n",
    "def tokenize_dataset(sentences, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes each Nepali sentence using the specified tokenizer.\n",
    "    \"\"\"\n",
    "    def tokenize_fn(text):\n",
    "        # Tokenizing the text using the provided tokenizer\n",
    "        tokens = tokenizer.encode(text)\n",
    "        return {\n",
    "            \"ids\": tokens.ids,\n",
    "            \"tokens\": tokens.tokens\n",
    "        }\n",
    "\n",
    "    # Tokenize each sentence and collect input_ids and tokens\n",
    "    tokenized_data = [tokenize_fn(sentence) for sentence in sentences]\n",
    "\n",
    "    # Separate ids and tokens into their respective lists\n",
    "    input_ids = [item['ids'] for item in tokenized_data]\n",
    "    tokens = [item['tokens'] for item in tokenized_data]\n",
    "\n",
    "    return input_ids, tokens\n",
    "\n",
    "\n",
    "    # Tokenize each sentence and collect input_ids and tokens\n",
    "    tokenized_data = [tokenize_fn(sentence) for sentence in sentences]\n",
    "\n",
    "    # Separate ids and tokens into their respective lists\n",
    "    input_ids = [item['ids'] for item in tokenized_data]\n",
    "    tokens = [item['tokens'] for item in tokenized_data]\n",
    "\n",
    "    return input_ids, tokens\n",
    "\n",
    "# Function to save tokenized data as .pkl\n",
    "def save_as_pkl(data, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Function to save vocabulary as .vocab\n",
    "def save_vocab(vocab, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for token in vocab:\n",
    "            f.write(f\"{token}\\n\")\n",
    "\n",
    "# Load Nepali text from a .txt file\n",
    "sentences = load_nepali_text_file(file_path)\n",
    "\n",
    "# Tokenize the dataset using SentencePiece\n",
    "input_ids_sp, tokens_sp = tokenize_dataset(sentences, tokenizer_sp)\n",
    "\n",
    "# Save tokenized data as .pkl\n",
    "save_as_pkl({\n",
    "    \"input_ids\": input_ids_sp,\n",
    "    \"tokens\": tokens_sp\n",
    "}, \"sentencepiece_tokenized.pkl\")\n",
    "\n",
    "# Save SentencePiece vocabulary\n",
    "sp_vocab = tokenizer_sp.get_vocab()  # Assuming get_vocab returns a list of vocabulary tokens\n",
    "save_vocab(sp_vocab, \"sentencepiece_vocab.vocab\")\n",
    "\n",
    "# Tokenize the dataset using WordPiece\n",
    "input_ids_wp, tokens_wp = tokenize_dataset(sentences, tokenizer_wp)\n",
    "\n",
    "# Save tokenized data as .pkl\n",
    "save_as_pkl({\n",
    "    \"input_ids\": input_ids_wp,\n",
    "    \"tokens\": tokens_wp\n",
    "}, \"wordpiece_tokenized.pkl\")\n",
    "\n",
    "# Save WordPiece vocabulary\n",
    "wp_vocab = tokenizer_wp.get_vocab()  # Assuming get_vocab returns a list of vocabulary tokens\n",
    "save_vocab(wp_vocab, \"wordpiece_vocab.vocab\")\n",
    "\n",
    "# Optionally, display a confirmation message\n",
    "print(\"Tokenized data and vocab files saved successfully.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenized IDs: [[35, 43, 1721, 14113, 9, 215, 2332, 7, 3591, 13277, 12, 3218, 25, 8962, 8652, 2894, 1028, 9169, 13, 7352, 1298, 1771, 111, 16441, 336, 135, 7, 24020, 8425, 2528, 687, 2189, 8425, 2014, 6789, 349, 9853, 768, 1771, 858, 1548, 2622, 26, 2216, 8425, 10, 1791, 43, 7690, 11002, 8992, 797, 10639, 8425, 2332, 36, 402, 203, 20938, 21659, 14713, 10, 44, 6870, 402, 1, 0], [7, 199, 2014, 6789, 349, 9853, 768, 1771, 858, 1548, 2622, 26, 2216, 8425, 10, 1791, 43, 7690, 11002, 8992, 797, 10639, 8425, 2332, 36, 402, 203, 20938, 21659, 14713, 10, 44, 6870, 402, 1, 0], [7, 199, 2014, 6789, 349, 9853, 768, 1771, 858, 1548, 2622, 26, 2216, 8425, 10, 1791, 43, 7690, 11002, 8992, 797, 10639, 8425, 2332, 36, 402, 203, 20938, 21659, 14713, 10, 44, 6870, 402, 1, 0], [7, 199, 2014, 6789, 349, 9853, 768, 1771, 858, 1548, 2622, 26, 2216, 8425, 10, 1791, 43, 7690, 11002, 8992, 797, 10639, 8425, 2332, 36, 402, 203, 20938, 21659, 14713, 10, 44, 6870, 402, 119, 7690, 11002, 1881, 145, 349, 1869, 3758, 9, 2121, 22, 46, 31, 402, 203, 14810, 3758, 4799, 367, 130, 47, 7, 24020, 8425, 10, 13, 1, 0], [119, 7690, 11002, 1881, 145, 349, 1869, 3758, 9, 2121, 22, 46, 31, 402, 203, 14810, 3758, 4799, 367, 130, 47, 7, 24020, 8425, 10, 13, 1, 0]]\n",
      "SentencePiece Tokens: [['▁यो', '▁एक', '▁स्वतन्त्र', 'विश्व', 'को', 'श', 'हो', '▁', 'जसलाई', 'सबै', 'ले', '▁सम्पादन', '▁गर्न', '▁सक्छन्।', 'नेपाली', '▁भाषामा', '▁कुल', '३१', ',', '३५', '०', '▁लेख', 'हरू', 'सङ्ग्रह', 'ित', '▁छन्।', '▁', 'देवनागरी', '▁लिपि', 'अथवा', 'ना', 'गरी', '▁लिपि', 'बा', 'याँ', '▁देखि', '▁दायाँ', '▁सम्म', '▁लेख', 'िने', 'अ', 'क्ष', 'र', 'ात्मक', '▁लिपि', 'मा', '▁आधारित', '▁एक', 'प्रा', 'चीन', '▁ब्रा', 'ह', '्मी', '▁लिपि', 'हो', '।', '▁यसको', '▁प्रयोग', 'भारतीय', '▁उपमहा', 'द्वीप', 'मा', '▁हुने', '▁गर्दछ।', '▁यसको', '<sep>', '<cls>'], ['▁', 'ि', 'बा', 'याँ', '▁देखि', '▁दायाँ', '▁सम्म', '▁लेख', 'िने', 'अ', 'क्ष', 'र', 'ात्मक', '▁लिपि', 'मा', '▁आधारित', '▁एक', 'प्रा', 'चीन', '▁ब्रा', 'ह', '्मी', '▁लिपि', 'हो', '।', '▁यसको', '▁प्रयोग', 'भारतीय', '▁उपमहा', 'द्वीप', 'मा', '▁हुने', '▁गर्दछ।', '▁यसको', '<sep>', '<cls>'], ['▁', 'ि', 'बा', 'याँ', '▁देखि', '▁दायाँ', '▁सम्म', '▁लेख', 'िने', 'अ', 'क्ष', 'र', 'ात्मक', '▁लिपि', 'मा', '▁आधारित', '▁एक', 'प्रा', 'चीन', '▁ब्रा', 'ह', '्मी', '▁लिपि', 'हो', '।', '▁यसको', '▁प्रयोग', 'भारतीय', '▁उपमहा', 'द्वीप', 'मा', '▁हुने', '▁गर्दछ।', '▁यसको', '<sep>', '<cls>'], ['▁', 'ि', 'बा', 'याँ', '▁देखि', '▁दायाँ', '▁सम्म', '▁लेख', 'िने', 'अ', 'क्ष', 'र', 'ात्मक', '▁लिपि', 'मा', '▁आधारित', '▁एक', 'प्रा', 'चीन', '▁ब्रा', 'ह', '्मी', '▁लिपि', 'हो', '।', '▁यसको', '▁प्रयोग', 'भारतीय', '▁उपमहा', 'द्वीप', 'मा', '▁हुने', '▁गर्दछ।', '▁यसको', '▁विकास', 'प्रा', 'चीन', '▁भारतमा', '▁पहिलो', '▁देखि', '▁चौथो', '▁शताब्दी', 'को', '▁बीचमा', '▁भएको', '▁थियो', '▁भने', '▁यसको', '▁प्रयोग', '▁सातौँ', '▁शताब्दी', '▁देखी', '▁हुँदै', '▁आएको', '▁छ।', '▁', 'देवनागरी', '▁लिपि', 'मा', ',', '<sep>', '<cls>'], ['▁विकास', 'प्रा', 'चीन', '▁भारतमा', '▁पहिलो', '▁देखि', '▁चौथो', '▁शताब्दी', 'को', '▁बीचमा', '▁भएको', '▁थियो', '▁भने', '▁यसको', '▁प्रयोग', '▁सातौँ', '▁शताब्दी', '▁देखी', '▁हुँदै', '▁आएको', '▁छ।', '▁', 'देवनागरी', '▁लिपि', 'मा', ',', '<sep>', '<cls>']]\n",
      "WordPiece Tokenized IDs: [[1, 8452, 8472, 10744, 8445, 10488, 8334, 5748, 9313, 13033, 5713, 15959, 8340, 14427, 8444, 11867, 1496, 8691, 13459, 10683, 5792, 5796, 16, 11341, 5723, 8892, 8429, 13816, 9802, 8371, 8425, 1496, 10072, 25694, 5712, 20967, 5757, 5763, 10478, 15925, 5712, 20967, 8392, 8585, 9267, 17488, 9279, 8892, 8492, 5757, 11339, 19392, 20967, 8337, 12152, 8472, 13896, 15893, 21336, 9148, 20967, 9313, 1496, 9640, 9074, 12487, 11450, 28511, 28072, 11098, 8511, 11322, 1496, 9640, 2], [1, 1464, 8392, 8585, 9267, 17488, 9279, 8892, 8492, 5757, 11339, 19392, 20967, 8337, 12152, 8472, 13896, 15893, 21336, 9148, 20967, 9313, 1496, 9640, 9074, 12487, 11450, 28511, 28072, 11098, 8511, 11322, 1496, 9640, 2], [1, 1464, 8392, 8585, 9267, 17488, 9279, 8892, 8492, 5757, 11339, 19392, 20967, 8337, 12152, 8472, 13896, 15893, 21336, 9148, 20967, 9313, 1496, 9640, 9074, 12487, 11450, 28511, 28072, 11098, 8511, 11322, 1496, 9640, 2], [1, 1464, 8392, 8585, 9267, 17488, 9279, 8892, 8492, 5757, 11339, 19392, 20967, 8337, 12152, 8472, 13896, 15893, 21336, 9148, 20967, 9313, 1496, 9640, 9074, 12487, 11450, 28511, 28072, 11098, 8511, 11322, 1496, 9640, 8781, 13896, 15893, 12104, 8904, 9267, 12241, 23826, 12536, 8428, 8491, 8417, 9640, 9074, 27424, 18539, 16007, 9453, 8950, 1430, 1496, 10072, 25694, 5712, 20967, 8337, 16, 2], [1, 8781, 13896, 15893, 12104, 8904, 9267, 12241, 23826, 12536, 8428, 8491, 8417, 9640, 9074, 27424, 18539, 16007, 9453, 8950, 1430, 1496, 10072, 25694, 5712, 20967, 8337, 16, 2]]\n",
      "WordPiece Tokens: [['[CLS]', 'यो', 'एक', 'स्वतन्त्र', '##वि', '##श्व', '##को', '##श', '##हो', 'जसलाई', '##स', '##बै', '##ले', 'सम्पादन', 'गर्न', 'सक्छन्', '।', 'नेपाली', 'भाषामा', 'कुल', '##३', '##१', ',', '३५', '##०', 'लेख', '##हरू', '##सङ्', '##ग्रह', '##ित', 'छन्', '।', 'देव', '##नागर', '##ी', 'लिपि', '##अ', '##थ', '##वान', '##ागर', '##ी', 'लिपि', '##बा', '##याँ', 'देखि', 'दायाँ', 'सम्म', 'लेख', '##िने', '##अ', '##क्षर', '##ात्मक', 'लिपि', '##मा', 'आधारित', 'एक', '##प्रा', '##चीन', 'ब्राह्', '##मी', 'लिपि', '##हो', '।', 'यसको', 'प्रयोग', '##भार', '##तीय', 'उपमहा', '##द्वी', '##पमा', 'हुने', 'गर्दछ', '।', 'यसको', '[SEP]'], ['[CLS]', 'ि', '##बा', '##याँ', 'देखि', 'दायाँ', 'सम्म', 'लेख', '##िने', '##अ', '##क्षर', '##ात्मक', 'लिपि', '##मा', 'आधारित', 'एक', '##प्रा', '##चीन', 'ब्राह्', '##मी', 'लिपि', '##हो', '।', 'यसको', 'प्रयोग', '##भार', '##तीय', 'उपमहा', '##द्वी', '##पमा', 'हुने', 'गर्दछ', '।', 'यसको', '[SEP]'], ['[CLS]', 'ि', '##बा', '##याँ', 'देखि', 'दायाँ', 'सम्म', 'लेख', '##िने', '##अ', '##क्षर', '##ात्मक', 'लिपि', '##मा', 'आधारित', 'एक', '##प्रा', '##चीन', 'ब्राह्', '##मी', 'लिपि', '##हो', '।', 'यसको', 'प्रयोग', '##भार', '##तीय', 'उपमहा', '##द्वी', '##पमा', 'हुने', 'गर्दछ', '।', 'यसको', '[SEP]'], ['[CLS]', 'ि', '##बा', '##याँ', 'देखि', 'दायाँ', 'सम्म', 'लेख', '##िने', '##अ', '##क्षर', '##ात्मक', 'लिपि', '##मा', 'आधारित', 'एक', '##प्रा', '##चीन', 'ब्राह्', '##मी', 'लिपि', '##हो', '।', 'यसको', 'प्रयोग', '##भार', '##तीय', 'उपमहा', '##द्वी', '##पमा', 'हुने', 'गर्दछ', '।', 'यसको', 'विकास', '##प्रा', '##चीन', 'भारतमा', 'पहिलो', 'देखि', 'चौथो', 'शताब्दीको', 'बीचमा', 'भएको', 'थियो', 'भने', 'यसको', 'प्रयोग', 'सातौँ', 'शताब्दी', 'देखी', 'हुँदै', 'आएको', 'छ', '।', 'देव', '##नागर', '##ी', 'लिपि', '##मा', ',', '[SEP]'], ['[CLS]', 'विकास', '##प्रा', '##चीन', 'भारतमा', 'पहिलो', 'देखि', 'चौथो', 'शताब्दीको', 'बीचमा', 'भएको', 'थियो', 'भने', 'यसको', 'प्रयोग', 'सातौँ', 'शताब्दी', 'देखी', 'हुँदै', 'आएको', 'छ', '।', 'देव', '##नागर', '##ी', 'लिपि', '##मा', ',', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "print(\"SentencePiece Tokenized IDs:\", input_ids_sp[:5])\n",
    "print(\"SentencePiece Tokens:\", tokens_sp[:5])\n",
    "print(\"WordPiece Tokenized IDs:\", input_ids_wp[:5])\n",
    "print(\"WordPiece Tokens:\", tokens_wp[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_model(dataset, tokenizer, model_name=\"xlm-roberta-base\", num_labels=2):\n",
    "    \"\"\"\n",
    "    Fine-tune a transformer-based model for Nepali text classification.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    print(\"Training complete!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model trained and saved as nepali_spm.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51389fd562444ae91051cb18169e642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a960b0a475974181bc7ca91b7116e5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "tokenize_dataset() got multiple values for argument 'tokenizer_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenize Dataset\u001b[39;00m\n\u001b[0;32m     14\u001b[0m spm_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspm_model_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m dataset, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspm_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnepali_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# # Train Transformer Model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_transformer_model(dataset, tokenizer)\n",
      "\u001b[1;31mTypeError\u001b[0m: tokenize_dataset() got multiple values for argument 'tokenizer_name'"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "from nepali_tokenizer import NepaliTokenizer\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    nepali_text_file = \"C:/Users/L E G I O N/Desktop/Nepali-LLM/data/merged_nepali_corpus.txt\"  # Path to a file with Nepali sentences\n",
    "    spm_model_prefix = \"nepali_spm\"\n",
    "\n",
    "    # Train SentencePiece Tokenizer\n",
    "    train_sentencepiece_tokenizer(nepali_text_file, spm_model_prefix)\n",
    "\n",
    "    # Load Dataset\n",
    "    dataset = load_nepali_dataset()\n",
    "\n",
    "    # Tokenize Dataset\n",
    "  \n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize the NepaliTokenizer\n",
    "tokenizer = NepaliTokenizer()\n",
    "\n",
    "# Function to load Nepali sentences from a .txt file\n",
    "def load_nepali_text_file(nepali_text_file):\n",
    "    \"\"\"\n",
    "    Load Nepali text from a .txt file and return a list of sentences.\n",
    "    \"\"\"\n",
    "    with open(nepali_text_file, 'r', encoding='utf-8') as f:\n",
    "        # Read all lines from the file\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Clean up the lines: Remove any unnecessary newlines and empty lines\n",
    "    sentences = [line.strip() for line in lines if line.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Function to tokenize the dataset using NepaliTokenizer\n",
    "def tokenize_dataset(sentences):\n",
    "    \"\"\"\n",
    "    Tokenizes each Nepali sentence using NepaliTokenizer.\n",
    "    \"\"\"\n",
    "    def tokenize_fn(text):\n",
    "        # Tokenizing the text using NepaliTokenizer\n",
    "        return tokenizer.tokenizer(text)\n",
    "    \n",
    "    # Tokenizing each sentence\n",
    "    tokenized_sentences = [tokenize_fn(sentence) for sentence in sentences]\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Load Nepali text from your local .txt file\n",
    "\n",
    "sentences = load_nepali_text_file(nepali_text_file)\n",
    "\n",
    "# Tokenize the sentences using NepaliTokenizer\n",
    "tokenized_sentences = tokenize_dataset(sentences)\n",
    "\n",
    "# Convert the tokenized data into a Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"text\": sentences, \"input_ids\": tokenized_sentences})\n",
    "\n",
    "# Optionally, display the first few entries of the dataset\n",
    "print(dataset[:5])  # Print first 5 tokenized entries\n",
    "\n",
    "#     # # Train Transformer Model\n",
    "# trained_model = train_transformer_model(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
