{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting indic-nlp-library\n",
      "  Using cached indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Downloading sphinx_rtd_theme-3.0.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Using cached Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinx-8.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: babel>=2.13 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.15.0)\n",
      "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests>=2.30.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l e g i o n\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.6.2)\n",
      "Using cached indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Using cached Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sphinx_rtd_theme-3.0.1-py2.py3-none-any.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.6/7.7 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.9/7.7 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.2/7.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.5/7.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.2/7.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.0/7.7 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.3/7.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "   ---------------------------------------- 0.0/587.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 587.4/587.4 kB 10.5 MB/s eta 0:00:00\n",
      "Downloading sphinx-8.1.3-py3-none-any.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.4/3.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 10.9 MB/s eta 0:00:00\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Downloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Installing collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, docutils, alabaster, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed alabaster-1.0.0 docutils-0.21.2 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 snowballstemmer-2.2.0 sphinx-8.1.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.1 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from indicnlp.tokenize import indic_tokenize, sentence_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters, numbers, and retain only Nepali characters\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', ' ', text)\n",
    "    \n",
    "    # Remove single Nepali characters\n",
    "    text = re.sub(r'\\s+[\\u0900-\\u097F]\\s+', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepali_stopwords = set([\n",
    "    'अक्सर', 'अगाडि', 'अझै', 'अनुसार', 'अन्तर्गत', 'अन्य', 'अन्यत्र', 'अन्यथा', 'अब', 'अरू', 'अरूलाई', \n",
    "    'अर्को', 'अर्थात', 'अर्थात्', 'अलग', 'आए', 'आजको', 'आठ', 'आत्म', 'आदि', 'आफू', 'आफूलाई', 'आफैलाई', \n",
    "    'आफ्नै', 'आफ्नो', 'आयो', 'उदाहरण', 'उन', 'उनको', 'उनले', 'उप', 'उहाँलाई', 'एउटै', 'एक', 'एकदम', \n",
    "    'औं', 'कतै', 'कम से कम', 'कसरी', 'कसै', 'कसैले', 'कहाँबाट', 'कहिलेकाहीं', 'कहिल्यै', 'कहीं', \n",
    "    'का', 'कि', 'किन', 'किनभने', 'कुनै', 'कुरा', 'कृपया', 'के', 'केहि', 'केही', 'को', 'कोही', 'क्रमशः', \n",
    "    'गए', 'गरि', 'गरी', 'गरेका', 'गरेको', 'गरेर', 'गरौं', 'गर्छ', 'गर्छु', 'गर्दै', 'गर्न', 'गर्नु', \n",
    "    'गर्नुपर्छ', 'गर्ने', 'गर्यौं', 'गैर', 'चाँडै', 'चार', 'चाले', 'चाहनुहुन्छ', 'चाहन्छु', 'चाहिए', 'छ', \n",
    "    'छन्', 'छु', 'छैन', 'छौँ', 'छौं', 'जताततै', 'जब', 'जबकि', 'जसको', 'जसबाट', 'जसमा', 'जसलाई', 'जसले', \n",
    "    'जस्तै', 'जस्तो', 'जस्तोसुकै', 'जहाँ', 'जान', 'जाहिर', 'जुन', 'जे', 'जो', 'ठीक', 'त', 'तत्काल', \n",
    "    'तथा', 'तदनुसार', 'तपाइँको', 'तपाईं', 'तर', 'तल', 'तापनि', 'तिनी', 'तिनीहरू', 'तिनीहरूको', 'तिनीहरूलाई', \n",
    "    'तिनीहरूले', 'तिमी', 'तिर', 'ती', 'तीन', 'तुरुन्तै', 'तेस्रो', 'त्यसकारण', 'त्यसपछि', 'त्यसमा', 'त्यसैले', \n",
    "    'त्यहाँ', 'त्यो', 'थिए', 'थिएन', 'थिएनन्', 'थियो', 'दिए', 'दिनुभएको', 'दिनुहुन्छ', 'दुई', 'देख', 'देखि', \n",
    "    'देखिन्छ', 'देखियो', 'देखे', 'देखेको', 'देखेर', 'देख्न', 'दोश्रो', 'दोस्रो', 'धेरै', 'न', 'नजिकै', 'नत्र', \n",
    "    'नयाँ', 'नि', 'निम्ति', 'निम्न', 'निम्नानुसार', 'निर्दिष्ट', 'नै', 'नौ', 'पक्का', 'पक्कै', 'पछि', \n",
    "    'पछिल्लो', 'पटक', 'पनि', 'पर्छ', 'पर्थ्यो', 'पर्याप्त', 'पहिले', 'पहिलो', 'पहिल्यै', 'पाँच', 'पाँचौं', \n",
    "    'पूर्व', 'प्रति', 'प्रत्येक', 'प्लस', 'फेरि', 'बने', 'बन्द', 'बन्न', 'बरु', 'बाटो', 'बारे', 'बाहिर', \n",
    "    'बाहेक', 'बीच', 'बीचमा', 'भए', 'भएको', 'भन', 'भने', 'भने्', 'भन्छन्', 'भन्छु', 'भन्दा', 'भन्नुभयो', \n",
    "    'भन्ने', 'भर', 'भित्र', 'भित्री', 'म', 'मलाई', 'मा', 'मात्र', 'माथि', 'मुख्य', 'मेरो', 'यति', \n",
    "    'यथोचित', 'यदि', 'यद्यपि', 'यस', 'यसको', 'यसपछि', 'यसबाहेक', 'यसरी', 'यसो', 'यस्तो', 'यहाँ', \n",
    "    'यहाँसम्म', 'या', 'यी', 'यो', 'र', 'रही', 'रहेका', 'रहेको', 'राखे', 'राख्छ', 'राम्रो', 'रूप', \n",
    "    'लगभग', 'लाई', 'लागि', 'ले', 'वरिपरि', 'वास्तवमा', 'वाहेक', 'विरुद्ध', 'विशेष', 'शायद', 'सँग', \n",
    "    'सँगै', 'सक्छ', 'सट्टा', 'सधैं', 'सबै', 'सबैलाई', 'समय', 'सम्भव', 'सम्म', 'सही', 'साँच्चै', \n",
    "    'सात', 'साथ', 'साथै', 'सायद', 'सारा', 'सो', 'सोध्न', 'सोही', 'स्पष्ट', 'हरे', 'हरेक', 'हामी', \n",
    "    'हामीलाई', 'हाम्रो', 'हुँ', 'हुन', 'हुने', 'हुनेछ', 'हुन्', 'हुन्छ', 'हो', 'होइन', 'होइनन्', 'होला', \n",
    "    'होस्'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('nepali'))\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(text, lang='ne'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords('text')\n",
    "    \n",
    "    # Return preprocessed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\L E G I O N/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\L E G I O N\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\L E G I O N/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\L E G I O N\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mनेपालको राजधानी काठमाण्डौ हो। यसमा विभिन्न ठाउँका मानिसहरू बस्छन्।\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed_text)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indic_tokenize\u001b[38;5;241m.\u001b[39mtrivial_tokenize(text, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mne\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mremove_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Return preprocessed text\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnepali\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\L E G I O N/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\L E G I O N\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text = \"नेपालको राजधानी काठमाण्डौ हो। यसमा विभिन्न ठाउँका मानिसहरू बस्छन्।\"\n",
    "preprocessed_text = preprocessing(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
